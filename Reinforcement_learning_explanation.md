**Task** 


The goal is to improve the bounding boxes or outlines generated by a model, specifically for outlining ships in satellite images. Initially, the idea was to use reinforcement learning to make ship classification in satellite images better. The plan was to train a model to detect ships and then use something like Grad-CAM to see which parts of the image the model focuses on when making its decision. These focused areas would be outlined to create a bounding box, which would then be adjusted using reinforcement learning to better match a ground truth polygon outlining the actual ship. The idea was that the model would focus more on the actual ship in the image, leading to more accurate predictions.

However, this approach seemed difficult to implement in practice. It’s hard to imagine how to accurately turn the parts of the image the model uses into a precise bounding box. So, a more practical approach was developed.

In this new plan, the ground truth polygons outlining the ships are created manually. Then, a model like Mask R-CNN is used to make an initial prediction of the ship’s outline. After that, reinforcement learning is applied, where each vertex of the predicted polygon is adjusted until it matches the ground truth polygon more closely. The aim is to show that there’s an improvement in performance when the model is trained with reinforcement learning. This can be tested by comparing the reinforcement learning model to the Mask R-CNN model that made the initial prediction.

The purpose of this report is to explain reinforcement learning, why it’s a good fit for this task, how it was used, and how this application is different from typical reinforcement learning tasks. The document bounding_analysis_working_doc.pdf explains the development process that led to the script used to generate the initial predicted and ground truth polygons, which were then used in the reinforcement learning script.


**Overview of Reinforcement Learning**


Reinforcement Learning (RL) allows for the agent to learn by interacting with the environment by making decisions. The agent aims to maximise cumulative reward over time by taking actions based on its observation of the environment. 
The environment is the ‘world’ in which the agent operates – essentially any scenario where decisions are made. An example could be a maze which a rover needs to navigate. 
The agent is the learner/ decision-maker. It observes the environment, takes actions and learns from the outcomes. 

It might be easier to explain methods in the following format:

Step (‘step’ method):
-	This is the code interaction between the agent and the environment. The agent takes an action, and the environment responds by returning:
o	Next state: the new situation the agent finds itself in after the action
o	Reward: A numerical value that tells the agent how good or bad the action was
o	Done: A Boolean flag indicating whether the episode (a complete sequence of actions) has ended
o	Info: Additional information that can be usedful for debugging or further analysis

Reset (‘reset’ method):
-	This method is used to start a new episode. It resets the environment to an initial state, providing the agent with the first observation. This is typically done after an episode ends, to begin learning again from a fresh state.
The agent uses rewards received after each step to learn which actions are good (should be repeated) and bad (should be avoided). Over time, the agent improves its policy, i.e. the strategy used to pick actions given a certain state.


**Why Reinforcement Learning is suited for this task**


It allows the model to explore different bounding box configurations and learn from its mistakes. By continuously refining its bounding boxes and receiving feedback on how well they fit the objects, the RL model can develop a robust policy that performs well across different satellite imagery.
It frames the bounding box adjustment as a sequential decision-making process, where each adjustment is an action taken by the agent in response to the current state of the bounding box. The model is trained to maximise the cumulative reward which ensures that the predicted polygon is as close to the ground truth as possible whilst still being a valid polygon. 
Simply put, by focusing on fine-tuning each vertex's position based on the satellite imagery, RL provides precise control, making it highly effective for achieving accurate and reliable object detection in complex visual environments.


**Best way to explain the application of RL is by describing the flow of code**


*Imports and Dependencies*
-	Imports necessary libraries such as Gymnasium (for RL environments), Shapely (for geometric operations) and Stable Baselines 3 (for RL algorithms – PPO is used in current script). 
-	Imports initially predicted polygons and ground reality polygons from a separate script so that these can be used for reinforcement learning

*‘PolygonEnv’ Class*
   
Initialisation:
-	This class is a custom Gym environment, initialised with imported polygons and the input image
-	The action space allows the agent to adjust the positions of these vertices.
-	The state (observation space) includes the current vertices of the polygon and the associated image.
  
‘reset’ method:
-	Initialises the environment at the start of each episode, resetting the polygon to its initial state and returning the initial observation

‘step’ method:
-	Agent carries out an action – observes the current state i.e. features from image and vertices of polygon, adjusts the vertices of the polygon accordingly, and computes the new state.
-	The reward is calculated according to the reward policy. 

Reward policy:
-	Involves maximising IoU value, in other words, maximising overlap between the predicted polygon and the ground truth polygon. 
-	To avoid the predicted polygon being changed so that it takes over the entire image to maximise overlap, a second factor is taken into consideration – penalisation of non-overlapping areas between the predicted and ground truth polygons. 
-	To ensure that the polygon is valid, i.e. no intersecting edges, a penalty was put into place every time an action causes the edges to intersect with each other. 
-	To ensure polygon stays within the bounds of the image. This penalty is a negative of the area of the polygon that is outside the image, which is then weighted by the magnitude of IoU so that this penalty doesn’t overpower other rewards.
-	This overall reward policy should ensure that the predicted polygon matches the shape of the ground truth as much as possible whilst remaining a valid polygon and staying within the image.



*‘CustomCNNFeatureExtractor’*
-	Custom feature extractor that uses a pre-trained ResNet18 model to extract features from the image, which are then used by the RL model to make decisions.


*Training Loop*

The main function sets up the training loop. The RL agent (using the PPO algorithm) iterates over multiple images, adjusting the polygon to maximize the reward.
Real-Time Visualization:
-	As the agent makes adjustments, the process is visualized in real time, showing the evolution of the polygon's shape as it learns.
Reward Logging:
-	Rewards are logged to track the agent's learning progress over time.


**How this script differs from typical reinforcement learning scripts**


In usual RL tasks, observations are simple (e.g. position and velocity in Catpole) and actions are discrete or low-dimensional. However, in this script, the observations include high-dimensional image data and vertices of the polygon. The action space is continuous and involves directly manipulating the vertices of a polygon, which is more complex than typical RL scenarios. 

Typical RL environments have straightforward reward (e.g. +1 for reaching a goal and -1 for failure) whereas this implementation considers IoU between polygons, penalties for non-overlapping areas as well as checking for polygon validity, making the termination condition more complex.


